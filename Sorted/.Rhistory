model2 <- lm(murders ~ execs, data=countymurders)
library(wooldridge)
data("countymurders")
model1 <- lm(countymurders$murders ~ countymurders$execs)
model2 <- lm(murders ~ execs, data=countymurders)
summary(model1)
model1$coefficients
abline(model1$coefficients[1])
abline(model1$coefficients[1], model1$coefficients[2])
library(wooldridge)
data("countymurders")
model1 <- lm(countymurders$murders ~ countymurders$execs)
model2 <- lm(murders ~ execs, data=countymurders)
summary(model1)
model1$coefficients
abline(a=model1$coefficients[1], b=model1$coefficients[2])
plot(model1)
library(wooldridge)
data("countymurders")
model1 <- lm(countymurders$murders ~ countymurders$execs)
model2 <- lm(murders ~ execs, data=countymurders)
summary(model1)
model1$coefficients
plot(countymurders$murders ~ countymurders$execs)
abline(a=model1$coefficients[1], b=model1$coefficients[2])
plot(model1)
model1 <- lm(countymurders$murders ~ countymurders$execs)
View(model1)
summary(model1)
library(wooldridge)
data("countymurders")
model1 <- lm(countymurders$murders ~ countymurders$execs)
summary(model1)
model2 <- lm(murders ~ execs, data=countymurders)
summary(model1)
model1$coefficients
plot(countymurders$murders ~ countymurders$execs)
abline(a=model1$coefficients[1], b=model1$coefficients[2]) ## can also use ggplot2 here
# plot(model1)
library(wooldridge)
library(dplyr)
fit<-lm(log(wage) ̃log(IQ),data=df_wage2)
summary(fit)
fit<-lm(log(wage)~̃log(IQ),data=df_wage2)
fit<-lm(log(wage)~log(IQ),data=df_wage2)
library(wooldridge)
library(dplyr)
fit<-lm(log(wage)~log(IQ),data=df_wage2)
summary(fit)
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readxl"), pkgTest)
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readr"), pkgTest)
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
View(hair_dryer)
###########################
# read in tsv dataset
###########################
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
View(hair_dryer)
View(hair_dryer)
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readr"), pkgTest)
###########################
# read in tsv dataset
###########################
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
################
###########################
# draw raw graph
###########################
hair_dryer %>%
filter(product_id=="R9T1FE2ZX2X04")
###########################
# draw raw graph
###########################
hair_dryer$product_id %>%
distinct()
###########################
# draw raw graph
###########################
hair_dryer$product_id %>%
distinct()
###########################
# draw raw graph
###########################
hair_dryer$product_id %>%
unique()
###########################
# draw raw graph
###########################
hair_dryer$product_parent %>%
unique()
###########################
# draw raw graph
###########################
hair_dryer$product_title %>%
unique()
###########################
# draw raw graph
###########################
hair_dryer$product_title %>%
unique() %>%
count()
###########################
# draw raw graph
###########################
title <- hair_dryer$product_title %>%
unique()
parent <- hair_dryer$product_parent %>%
unique()
parent <- hair_dryer$product_id %>%
unique()
libray(ggplot2)
library(ggplot2)
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readr", "ggplot2"), pkgTest)
###########################
# read in tsv dataset
###########################
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
###########################
# draw raw graph
###########################
id <- hair_dryer$product_id %>%
unique()
View(hair_dryer)
View(hair_dryer)
hair_dryer %>%
group_by(product_id, review_date) %>%
count()
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readr", "ggplot2"), pkgTest)
###########################
# read in tsv dataset
###########################
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
###########################
# draw raw graph
###########################
id <- hair_dryer$product_id %>%
unique()
hair_dryer %>%
group_by(product_id, review_date) %>%
count()
hair_dryer %>%
group_by(product_id, review_date) %>%
mutate(number = count())
############################
# clean environment
# load packages
############################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("stringr", "dplyr", "plyr", "tidyverse", "rvest", "zoo", "XML", "tidyr", "lubridate", "readr", "ggplot2"), pkgTest)
###########################
# read in tsv dataset
###########################
hair_dryer <- read.delim2("~/Desktop/2020_Weekend2_Problems/Problem_C_Data/hair_dryer.tsv")
###########################
# draw raw graph
###########################
id <- hair_dryer$product_id %>%
unique()
df <- hair_dryer %>%
group_by(product_id, review_date) %>%
count
View(df)
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Lu wd
setwd('~/Documents/GitHub/MCM2020E/sorted/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# read in the sorted dataset
speechesDF <- read.csv("hair_dryer_sorted.csv", stringsAsFactors = F, encoding = "UTF-8")
View(speechesDF)
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Lu wd
setwd('~/Documents/GitHub/MCM2020E/sorted/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# read in the sorted dataset
speechesDF <- read.csv("hair_dryer_sorted.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
# create function to clean speeches
# to create DTM
###################################
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of speeches
for(row in 1:dim(speechesDF)[1]){
all_words <- rbind(all_words, clean_text(speechesDF[row, "review_body"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
View(all_words)
#######################
# set working directory
# load data
# and load libraries
#######################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages()
# Lu wd
setwd('~/Documents/GitHub/MCM2020E/sorted/')
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
lapply(c("quanteda", "stringr", "tm"), pkgTest)
# read in the sorted dataset
hair_dryer_DF <- read.csv("hair_dryer_sorted.csv", stringsAsFactors = F, encoding = "UTF-8")
###################################
# create function to clean hair_dryers
# to create DTM
###################################
clean_text <- function(inputVec){
# lowercase
tempVec <- tolower(inputVec)
# remove everything that is not a number or letter
tempVec <- str_replace_all(tempVec,"[^a-zA-Z\\s]", " ")
# make sure all spaces are just one white space
tempVec <- str_replace_all(tempVec,"[\\s]+", " ")
# remove blank words
tempVec <- tempVec[which(tempVec!="")]
#browser()
# tokenize (split on each word)
tempVec <- str_split(tempVec, " ")[[1]]
# create function for removing stop words
remove_words <- function(str, stopwords) {
x <- unlist(strsplit(str, " "))
x <- x[!x %in% stopwords]
# remove single letter words
return(x[nchar(x) > 1])
}
# remove stop words
tempVec <- remove_words(tempVec, stopwords("english"))
# get count of each word in "document"
count_df <- data.frame(document=row,
count=rle(sort(tempVec))[[1]],
word=rle(sort(tempVec))[[2]])
return(count_df)
}
# create new vector that we will continuously append via rbind
# probably not the most computationally efficient way to do this...
all_words <- NULL
# loop over all rows in original DF of hair_dryers
for(row in 1:dim(hair_dryer_DF)[1]){
all_words <- rbind(all_words, clean_text(hair_dryer_DF[row, "review_body"]))
}
# find unique words in word matrix
unique(all_words$word)
# there are 676 unique words in corpus
###################################
# create open DTM filled w/ zeroes
###################################
DTM <- matrix(0, nrow=dim(hair_dryer_DF)[1], ncol=length(unique(all_words$word)))
# assign column names of DTM to be the unique words (in alpha order)
colnames(DTM) <- unique(all_words$word)
# loop over each "document"/paragraph
for(document in 1:dim(hair_dryer_DF)[1]){
# find all the words that are used in that paragraph
document_subset <- all_words[which(all_words$document==document),]
# loop over each word
for(row in 1:dim(document_subset)[1]){
# and check which column it's in
DTM[document, which(colnames(DTM)==document_subset[row, "word"] )] <- all_words[row, "count"]
}
}
# compare how we did
# first look at DTM for 50th observation
which(DTM[50,]>0)
hair_dryer_DF[50,"paragraph_text"]
##################################
# Doing this all using library(tm)
##################################
# create new corpus object with tm
hair_dryer_corpus <- Corpus(VectorSource(hair_dryer_DF$paragraph_text))
# clean corpus as we did before
hair_dryer_corpus <- tm_map(hair_dryer_corpus, content_transformer(tolower))
hair_dryer_corpus <- tm_map(hair_dryer_corpus, removeNumbers)
hair_dryer_corpus <- tm_map(hair_dryer_corpus, removePunctuation)
hair_dryer_corpus <- tm_map(hair_dryer_corpus, removeWords, c("the", "and", stopwords("english")))
hair_dryer_corpus <- tm_map(hair_dryer_corpus, stripWhitespace)
# create DTM
alternative_DTM <- DocumentTermMatrix(hair_dryer_corpus)
# check to see what it looks like
inspect(alternative_DTM[1:50, 1:50])
norm_vec <- function(x) {
sqrt(sum(x^2))
}
euclidean <- function(x,y) {
sqrt(sum((x-y)^2))
}
cosine <- function(x,y) {
(x/norm_vec(x)) %*% (y/norm_vec(y))
}
x <- c(1,0,1)
y <- c(1/2,0,1/2)
euclidean(x,y)
cosine(x,y)
####################
# TF-IDF
####################
# 2019/11/08
# check if there is 0 inside the document
sum(colSums(DTM)==0)
# TF-IDF
TF <- t(DTM)
IDF <- log(ncol(TF)/(rowSums(TF)))
IDF <- diag(IDF)
TF_IDF = crossprod(TF, IDF)
colnames(TF_IDF) <- rownames(TF)
# normalize TF-IDF
TF_IDF_normalized <- TF_IDF/sqrt(rowSums(TF_IDF^2))
# distance matrix
# basically re-arrange TF-IDF
